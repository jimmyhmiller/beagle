namespace slow_json_parser

import "beagle.primitive" as primitive

struct Tokenizer {
    input
    mut position
    length
}

fn make_tokenizer(input) {
    Tokenizer {
        input: input,
        position: 0,
        length: length(input)
    }
}

fn peek(tokenizer) {
    if tokenizer.position >= tokenizer.length {
        ""
    } else {
        let result = tokenizer.input[tokenizer.position]
        result
    }
}

fn consume(tokenizer) {
    primitive/set!(tokenizer.position, tokenizer.position + 1)
    tokenizer
}

fn skip_whitespace(tokenizer) {
    if tokenizer.position >= tokenizer.length {
        tokenizer
    } else {
        let c = peek(tokenizer)
        if c == " " || c == "\n" || c == "\t" || c == "\r" {
            skip_whitespace(consume(tokenizer))
        } else {
            tokenizer
        }
    }
}

fn find_string_end(tokenizer) {
    if tokenizer.position >= tokenizer.length {
        tokenizer.position
    } else {
        let c = peek(tokenizer)
        if c == "\"" {
            tokenizer.position
        } else {
            find_string_end(consume(tokenizer))
        }
    }
}

fn parse_string(tokenizer) {
    consume(tokenizer)
    let start_pos = tokenizer.position
    let end_pos = find_string_end(tokenizer)
    let str_val = substring(tokenizer.input, start_pos, end_pos - start_pos)
    if tokenizer.position < tokenizer.length {
        consume(tokenizer)
    }
    str_val
}

fn is_digit_char(c) {
    c == "0" || 
    c == "1" || 
    c == "2" || 
    c == "3" || 
    c == "4" || 
    c == "5" || 
    c == "6" || 
    c == "7" || 
    c == "8" ||
    c == "9"
}

fn find_number_end(tokenizer) {
    if tokenizer.position >= tokenizer.length {
        tokenizer.position
    } else {
        let c = peek(tokenizer)
        if is_digit_char(c) || c == "." || c == "-" {
            find_number_end(consume(tokenizer))
        } else {
            tokenizer.position
        }
    }
}

fn parse_number(tokenizer) {
    let start_pos = tokenizer.position
    let end_pos = find_number_end(tokenizer)
    let number_string = substring(tokenizer.input, start_pos, end_pos - start_pos)
    to-number(number_string)
}

fn find_word_end(tokenizer) {
    if tokenizer.position >= tokenizer.length {
        tokenizer.position
    } else {
        let c = peek(tokenizer)
        if c >= "a" && c <= "z" {
            find_word_end(consume(tokenizer))
        } else {
            tokenizer.position
        }
    }
}

fn parse_true(tokenizer) {
    consume(tokenizer)
    true
}

fn parse_false(tokenizer) {
    primitive/set!(tokenizer.position, tokenizer.position + 5)
    false
}

fn parse_null(tokenizer) {
    primitive/set!(tokenizer.position, tokenizer.position + 4)
    null
}

fn parse_value(tokenizer) {
    let tokenizer = skip_whitespace(tokenizer)
    let c = peek(tokenizer)
    if c == "{" {
        parse_object(tokenizer)
    } else if c == "[" {
        parse_array(tokenizer)
    } else if c == "\"" {
        parse_string(tokenizer)
    } else if c == "t" {
        parse_true(tokenizer)
    } else if c == "f" {
        parse_false(tokenizer)
    } else if c == "n" {
        parse_null(tokenizer)
    } else {
        parse_number(tokenizer)
    }
}

fn parse_object(tokenizer) {
    consume(tokenizer)
    let tokenizer = skip_whitespace(tokenizer)
    if peek(tokenizer) == "}" {
        consume(tokenizer)
        pm/map()
    } else {
        parse_object_loop(tokenizer, pm/map())
    }
}

fn parse_object_loop(tokenizer, out_map) {
    let tokenizer = skip_whitespace(tokenizer)
    let key = parse_string(tokenizer)
    let after_key = skip_whitespace(tokenizer)
    consume(after_key)
    let after_colon = skip_whitespace(after_key)
    let val = parse_value(after_colon)
    let new_map = pm/assoc(out_map, key, val)
    let after_val = skip_whitespace(after_colon)
    let c = peek(after_val)
    if c == "," {
        consume(after_val)
        parse_object_loop(skip_whitespace(after_val), new_map)
    } else {
        if c == "}" {
            consume(after_val)
        }
        new_map
    }
}

fn parse_array(tokenizer) {
    consume(tokenizer)
    let tokenizer = skip_whitespace(tokenizer)
    if peek(tokenizer) == "]" {
        consume(tokenizer)
        pv/vec()
    } else {
        parse_array_loop(tokenizer, pv/vec())
    }
}

fn parse_array_loop(tokenizer, out_vec) {
    let tokenizer = skip_whitespace(tokenizer)
    let val = parse_value(tokenizer)
    let new_vec = pv/push(out_vec, val)
    let after_val = skip_whitespace(tokenizer)
    let c = peek(after_val)
    if c == "," {
        consume(after_val)
        parse_array_loop(skip_whitespace(after_val), new_vec)
    } else {
        if c == "]" {
            consume(after_val)
        }
        new_vec
    }
}

fn parse(input) {
    let tokenizer = make_tokenizer(input)
    parse_value(tokenizer)
}


fn main() {
    let input = "{\"a\": [1, 2, 3], \"b\": {\"c\": 4, \"d\": 5}, \"e\": true, \"f\": false, \"g\": null}"
    let output = parse(input)
    println(output)
    "done"
}